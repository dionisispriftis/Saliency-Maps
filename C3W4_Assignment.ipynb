{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C3W4_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dionisispriftis/Saliency-Maps/blob/main/C3W4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQiSujBfjWj"
      },
      "source": [
        "# **Week 4 Assignment: Saliency Maps**\n",
        "\n",
        "A saliency map shows the pixels which greatly impacts the classification of an image. \n",
        "- This is done by getting the gradient of the loss with respect to changes in the pixel values, then plotting the results. \n",
        "- From there, you can see if your model is looking at the correct features when classifying an image. \n",
        "  - For example, if you're building a dog breed classifier, you should be wary if your saliency map shows strong pixels outside the dog itself (e.g. sky, grass, dog house, etc...).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDHISSfBq40T"
      },
      "source": [
        "### Download test files and weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Laatr1c6lr1w"
      },
      "source": [
        "# Download the same test files from the Cats vs Dogs ungraded lab\n",
        "!wget -O cat1.jpg https://storage.googleapis.com/laurencemoroney-blog.appspot.com/MLColabImages/cat1.jpg\n",
        "!wget -O cat2.jpg https://storage.googleapis.com/laurencemoroney-blog.appspot.com/MLColabImages/cat2.jpg\n",
        "!wget -O catanddog.jpg https://storage.googleapis.com/laurencemoroney-blog.appspot.com/MLColabImages/catanddog.jpg\n",
        "!wget -O dog1.jpg https://storage.googleapis.com/laurencemoroney-blog.appspot.com/MLColabImages/dog1.jpg\n",
        "!wget -O dog2.jpg https://storage.googleapis.com/laurencemoroney-blog.appspot.com/MLColabImages/dog2.jpg\n",
        "\n",
        "# Download prepared weights\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kipXTxesGJKGY1B8uSPRvxROgOH90fih' -O 0_epochs.h5\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1oiV6tjy5k7h9OHGTQaf0Ohn3FmF-uOs1' -O 15_epochs.h5\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=14vFpBJsL_TNQeugX8vUTv8dYZxn__fQY' -O 95_epochs.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24L3lKwqb3E"
      },
      "source": [
        "### Import the required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X86LKLvpBO2S"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th4dA3I8-9Ue"
      },
      "source": [
        "### Download and prepare the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1hujOK9rDyU"
      },
      "source": [
        "#### Load Cats vs Dogs \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w5HNdoHBQv_"
      },
      "source": [
        "# Load the data and create the train set (optional: val and test sets)\n",
        "\n",
        "train_ds = tfds.load('cats_vs_dogs', split='train[:80%]', as_supervised = True)\n",
        "validation_ds = tfds.load('cats_vs_dogs', split='train[80%:90%]', as_supervised = True)\n",
        "test_ds = tfds.load('cats_vs_dogs', split='train[90%:]', as_supervised = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXp0mV5Rbo76"
      },
      "source": [
        "#### Create preprocessing function\n",
        "\n",
        "This function will:\n",
        "  * cast the image to float32\n",
        "  * normalize the pixel values to [0, 1]\n",
        "  * resize the image to 300 x 300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRkrL2aK2_UZ"
      },
      "source": [
        "def augment_train_images(image, label):\n",
        "  \n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.0\n",
        "  image = tf.image.resize(image, (300, 300))\n",
        "\n",
        "  return image, label\n",
        "\n",
        "def augment_test_images(image, label):\n",
        "  \n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.0\n",
        "  image = tf.image.resize(image, (300, 300))\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "  label = tf.expand_dims(label, axis=0)\n",
        "  print(label.shape)\n",
        "\n",
        "  return image, label "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzvF61GV32k_"
      },
      "source": [
        "#### Preprocess the training set\n",
        "\n",
        "We use the `map()` and pass in the method that we just defined to preprocess the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpNEfDKM353a"
      },
      "source": [
        "augmented_training_data = train_ds.map(augment_train_images)\n",
        "augmented_validation_data = validation_ds.map(augment_test_images)\n",
        "augmented_test_data = test_ds.map(augment_test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4nFaMIMbrvA"
      },
      "source": [
        "#### Create batches of the training set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POhDDPBY3vnL"
      },
      "source": [
        "train_batches = augmented_training_data.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za5HxgT1_Cw6"
      },
      "source": [
        "### Build the Cats vs Dogs classifier \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7_Z8NWDkYob"
      },
      "source": [
        "def conv_block(input, filters, global_pooling = False):\n",
        "\n",
        "  x = Conv2D(filters=filters,kernel_size=(3,3),activation='relu',padding='same')(input)\n",
        "  if(global_pooling):\n",
        "    return GlobalAveragePooling2D()(x)\n",
        "  else:\n",
        "    return MaxPooling2D(pool_size=(2,2))(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoyCA80GBSlG"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D\n",
        "\n",
        "input = tf.keras.Input(shape=(300, 300, 3))\n",
        "x = conv_block(input, 16)\n",
        "x = conv_block(x, 32)\n",
        "x = conv_block(x, 64)\n",
        "x = conv_block(x, 128, True)\n",
        "output = Dense(2,activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6nou82P_b5d"
      },
      "source": [
        "### Create a function to generate the saliency map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKbvh3bl9vnG"
      },
      "source": [
        "def do_salience(image, model, label, prefix):\n",
        "  '''\n",
        "  Generates the saliency map of a given image.\n",
        "\n",
        "  Args:\n",
        "    image (file) -- picture that the model will classify\n",
        "    model (keras Model) -- your cats and dogs classifier\n",
        "    label (int) -- ground truth label of the image\n",
        "    prefix (string) -- prefix to add to the filename of the saliency map\n",
        "  '''\n",
        "\n",
        "  # Read the image and convert channel order from BGR to RGB\n",
        "  TEST_IMAGES_PATH = '/content'\n",
        "  num_classes = 2\n",
        "\n",
        "\n",
        "  img = cv2.imread(TEST_IMAGES_PATH + '/' + image )\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (300, 300)) / 255\n",
        "\n",
        "  # Add an additional dimension (for the batch), and save this in a new variable\n",
        "  tensor_image = tf.expand_dims(img, axis = 0)\n",
        "  \n",
        "  # Define the expected output array by one-hot encoding the label\n",
        "  # The length of the array is equal to the number of classes\n",
        "  expected_output = tf.one_hot([label] * tensor_image.shape[0], num_classes)\n",
        "  tensor_image = tf.cast(tensor_image, tf.float32)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(tensor_image)\n",
        "    y_pred = model(tensor_image)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_pred, expected_output)\n",
        "    print('prediction:', y_pred), print('loss:', loss)\n",
        "    \n",
        "  gradients = tape.gradient(loss, tensor_image)\n",
        "    \n",
        "  # generate the grayscale tensor\n",
        "  grayscale_tensor = tf.reduce_sum(tf.abs(gradients), axis=-1)\n",
        "\n",
        "  # normalize the pixel values to be in the range [0, 255].\n",
        "  # Cast the tensor as a tf.uint8\n",
        "\n",
        "  max, min = tf.reduce_max(grayscale_tensor), tf.reduce_min(grayscale_tensor)\n",
        "  normalized_tensor = 255 * (grayscale_tensor - min) / (max - min)\n",
        "  normalized_tensor = tf.cast(normalized_tensor, tf.uint8)  \n",
        "\n",
        "  # Remove dimensions that are size 1\n",
        "  normalized_tensor = tf.squeeze(normalized_tensor)\n",
        "    \n",
        "  # plot the normalized tensor and the superimposed saliency map to the original image  \n",
        "  fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
        "  \n",
        "  gradient_color = cv2.applyColorMap(normalized_tensor.numpy(), cv2.COLORMAP_HOT)\n",
        "  gradient_color = gradient_color / 255.0\n",
        "  super_imposed = cv2.addWeighted(img, 0.5, gradient_color, 0.5, 0.0)\n",
        "  \n",
        "  ax[0].axis('off'), ax[1].axis('off')\n",
        "  ax[0].imshow(normalized_tensor, cmap='gray'), ax[1].imshow(super_imposed)\n",
        "  plt.show()\n",
        "\n",
        "  # save the normalized tensor image to a file.\n",
        "  salient_image_name = prefix + image\n",
        "  normalized_tensor = tf.expand_dims(normalized_tensor, -1)\n",
        "  normalized_tensor = tf.io.encode_jpeg(normalized_tensor, quality=100, format='grayscale')\n",
        "  writer = tf.io.write_file(salient_image_name, normalized_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZhZgd0x_JvN"
      },
      "source": [
        "### Configure the model for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkyWZ5KdBo-z"
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=opt,\n",
        "              loss=loss, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIoJJw7_ZFN"
      },
      "source": [
        "### Train your model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcLNUyG-cTxo"
      },
      "source": [
        "def generate_saliency(filenames, prefix):\n",
        "\n",
        "  for name in filenames:\n",
        "  label = 1\n",
        "  if name[0:3]=='cat':\n",
        "    label = 0\n",
        "  print(name)\n",
        "  do_salience(name, model, label, prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YSNp7k7BqfL"
      },
      "source": [
        "FILENAMES = ['cat1.jpg','cat2.jpg','catanddog.jpg','dog1.jpg','dog2.jpg']\n",
        "EPOCHS = 10\n",
        "\n",
        "# load pre-trained weights\n",
        "model.load_weights('15_epochs.h5')\n",
        "model.fit(train_batches, epochs=3, validation_data=augmented_validation_data)\n",
        "generate_saliency(FILENAMES, 'epoch25_salient')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k39fF4n8fgG0"
      },
      "source": [
        "model.load_weights('95_epochs.h5')\n",
        "\n",
        "# generate the saliency maps for the 5 test images\n",
        "generate_saliency(FILENAMES, 'epoch95_salient')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}